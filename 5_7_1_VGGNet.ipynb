{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imhyunho99/2023-1--Deaplearning_Framework/blob/main/5_7_1_VGGNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "BZiTXRcBHyFo"
      },
      "source": [
        "# VGGNet Implementation\n",
        "\n",
        "- 컨볼루션 연산만 배운 상태에서 VGG를 바로 이해하고 짜기에는 무리가 있습니다.\n",
        "- 연산들의 동작 원리를 충분히 이해한후 다시 보셔도 늦지 않습니다.\n",
        "\n",
        "- 2014 ILSVRC 2nd place\n",
        "- VGG-16\n",
        "- Convolution layer\n",
        "- Maxpooling layer\n",
        "- Fully connected layer\n",
        "\n",
        "![대체 텍스트](https://qph.fs.quoracdn.net/main-qimg-e657c195fc2696c7d5fc0b1e3682fde6)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-C-HidGIaAC",
        "outputId": "54b89edf-f8e5-4a43-89c3-08741c1054d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 런타임 유형 GPU 모드로 변경\n",
        "!pip install torch torchvision"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (0.15.1+cu118)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch) (3.10.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch) (3.0)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (16.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz9l06K4eVE8"
      },
      "source": [
        "## Prepare Data\n",
        "\n",
        "- 모델이 학습이 되는지만 확인할 수 있게 간단한 데이터를 다운로드 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xU1Z6btAPFoO",
        "outputId": "6f975d39-c982-47a4-ea10-aa640b25ad58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!rm -r images\n",
        "import os \n",
        "\n",
        "# 이미지 파일을 저장할 폴더를 생성합니다.\n",
        "try:\n",
        "  os.mkdir(\"images\")\n",
        "  os.mkdir(\"images/dogs\")\n",
        "  os.mkdir(\"images/cats\")\n",
        "except:\n",
        "  pass\n",
        "\n",
        "# 이미지들을 지정한 위치에 다운로드합니다.\n",
        "# images/dogs 밑에 2개\n",
        "!wget https://i.kinja-img.com/gawker-media/image/upload/s--WFkXeene--/c_scale,f_auto,fl_progressive,q_80,w_800/ol9ceoqxidudap8owlwn.jpg -P images/dogs\n",
        "!wget https://www.rspcansw.org.au/wp-content/uploads/2017/08/50_a-feature_dogs-and-puppies_mobile.jpg -P images/dogs\n",
        "\n",
        "# images/cats 밑에 2개\n",
        "!wget https://www.catster.com/wp-content/uploads/2018/05/A-gray-cat-crying-looking-upset.jpg -P images/cats\n",
        "!wget https://www.scarymommy.com/wp-content/uploads/2018/01/c1.jpg?w=700 -P images/cats"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-10 05:29:41--  https://i.kinja-img.com/gawker-media/image/upload/s--WFkXeene--/c_scale,f_auto,fl_progressive,q_80,w_800/ol9ceoqxidudap8owlwn.jpg\n",
            "Resolving i.kinja-img.com (i.kinja-img.com)... 151.101.130.166, 151.101.66.166, 151.101.2.166, ...\n",
            "Connecting to i.kinja-img.com (i.kinja-img.com)|151.101.130.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 36509 (36K) [image/jpeg]\n",
            "Saving to: ‘images/dogs/ol9ceoqxidudap8owlwn.jpg’\n",
            "\n",
            "\rol9ceoqxidudap8owlw   0%[                    ]       0  --.-KB/s               \rol9ceoqxidudap8owlw 100%[===================>]  35.65K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2023-04-10 05:29:41 (37.0 MB/s) - ‘images/dogs/ol9ceoqxidudap8owlwn.jpg’ saved [36509/36509]\n",
            "\n",
            "--2023-04-10 05:29:41--  https://www.rspcansw.org.au/wp-content/uploads/2017/08/50_a-feature_dogs-and-puppies_mobile.jpg\n",
            "Resolving www.rspcansw.org.au (www.rspcansw.org.au)... 101.0.86.38\n",
            "Connecting to www.rspcansw.org.au (www.rspcansw.org.au)|101.0.86.38|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 130940 (128K) [image/jpeg]\n",
            "Saving to: ‘images/dogs/50_a-feature_dogs-and-puppies_mobile.jpg’\n",
            "\n",
            "50_a-feature_dogs-a 100%[===================>] 127.87K   158KB/s    in 0.8s    \n",
            "\n",
            "2023-04-10 05:29:44 (158 KB/s) - ‘images/dogs/50_a-feature_dogs-and-puppies_mobile.jpg’ saved [130940/130940]\n",
            "\n",
            "--2023-04-10 05:29:44--  https://www.catster.com/wp-content/uploads/2018/05/A-gray-cat-crying-looking-upset.jpg\n",
            "Resolving www.catster.com (www.catster.com)... 18.65.39.96, 18.65.39.67, 18.65.39.80, ...\n",
            "Connecting to www.catster.com (www.catster.com)|18.65.39.96|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 165145 (161K) [image/jpeg]\n",
            "Saving to: ‘images/cats/A-gray-cat-crying-looking-upset.jpg’\n",
            "\n",
            "A-gray-cat-crying-l 100%[===================>] 161.27K  --.-KB/s    in 0.009s  \n",
            "\n",
            "2023-04-10 05:29:44 (18.2 MB/s) - ‘images/cats/A-gray-cat-crying-looking-upset.jpg’ saved [165145/165145]\n",
            "\n",
            "--2023-04-10 05:29:44--  https://www.scarymommy.com/wp-content/uploads/2018/01/c1.jpg?w=700\n",
            "Resolving www.scarymommy.com (www.scarymommy.com)... 18.154.63.41, 18.154.63.7, 18.154.63.125, ...\n",
            "Connecting to www.scarymommy.com (www.scarymommy.com)|18.154.63.41|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2023-04-10 05:29:44 ERROR 404: Not Found.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "ZELujmpnHyFq"
      },
      "source": [
        "## 1. Settings\n",
        "### 1) Import required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqkPfR0oHyFq"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.init as init\n",
        "import torch.utils.data as data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-umoG8-dHyFw"
      },
      "source": [
        "### 2) Hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tahoG-3cHyFx"
      },
      "source": [
        "batch_size= 1\n",
        "learning_rate = 0.0002\n",
        "num_epoch = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgvQYlzOHyFz"
      },
      "source": [
        "## 2. Data Loader\n",
        "\n",
        "- https://pytorch.org/docs/stable/torchvision/datasets.html?highlight=imagefolder#torchvision.datasets.ImageFolder\n",
        "- ImageFolder라는 함수를 이용해 따로 이미지-라벨 쌍을 만들지 않고 폴더에 저장하는것만으로 쉽게 이미지-라벨 쌍을 만들 수 있습니다.\n",
        "\n",
        "ex)\n",
        "\n",
        "root/dog/xxx.png\n",
        "\n",
        "root/dog/xxy.png\n",
        "\n",
        "root/cat/123.png\n",
        "\n",
        "root/cat/nsdf3.png\n",
        "\n",
        "                    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWItBFtRJ_Rx"
      },
      "source": [
        "# 라벨(혹은 클래스) 별로 폴더가 저장되어 있는 루트 디렉토리를 지정합니다.\n",
        "img_dir = \"./images\"\n",
        "\n",
        "# 해당 루트 디렉토리를 ImageFolder 함수에 전달합니다.\n",
        "# 이때 이미지들에 대한 변형도 같이 전달해줍니다.\n",
        "img_data = dset.ImageFolder(img_dir, transforms.Compose([\n",
        "                                      transforms.Resize(256),                   # 이미지 크기를 256x256으로 바꿔줍니다.\n",
        "                                      transforms.RandomResizedCrop(224),        # 256x256 이미지의 랜덤한 위치에서 224x224 크기만큼 샘플링 합니다.\n",
        "                                      transforms.RandomHorizontalFlip(),        # 랜덤한 확률로 이미지를 좌우반전 합니다.\n",
        "                                      transforms.ToTensor(),                    # 이미지 데이터를 텐서로 변형합니다.\n",
        "            ]))\n",
        "\n",
        "train_loader = data.DataLoader(img_data, batch_size=batch_size,\n",
        "                            shuffle=True, num_workers=2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CABXOGJxHyF4"
      },
      "source": [
        "## 3. Model \n",
        "### 1) Basic Blocks\n",
        "\n",
        "- 모델에 반복되는 부분이 많기 때문에 이를 함수로 만들어 단순화 합니다.\n",
        "- 맨 위에 이미지를 보면 컨볼루션 연산이 2번 연속하는 경우와 3번 연속하는 경우가 있는데 이를 각각 만들어줍니다.\n",
        "- 아래의 코드는 최적의 방법이라기 보다는 그림의 구조를 모방한 코드입니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EOMiN5iHyF5"
      },
      "source": [
        "# 컨볼루션 연산이 2번 연속하는 경우\n",
        "# 컨볼루션-활성화함수-컨볼루션-활성화함수-풀링\n",
        "def conv_2_block(in_dim,out_dim):\n",
        "    model = nn.Sequential(\n",
        "        nn.Conv2d(in_dim,out_dim,kernel_size=3,padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(out_dim,out_dim,kernel_size=3,padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2,2)\n",
        "    )\n",
        "    return model\n",
        "\n",
        "  \n",
        "# 컨볼루션 연산이 3번 연속하는 경우==>4번, conv_4_block로 변경\n",
        "# 컨볼루션-활성화함수-컨볼루션-활성화함수-컨볼루션-활성화함수-풀링\n",
        "def conv_4_block(in_dim,out_dim):\n",
        "    model = nn.Sequential(\n",
        "        nn.Conv2d(in_dim,out_dim,kernel_size=3,padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(out_dim,out_dim,kernel_size=3,padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(out_dim,out_dim,kernel_size=3,padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(out_dim,out_dim,kernel_size=3,padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2,2)\n",
        "    )\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoDJbUUVHyF8"
      },
      "source": [
        "### 2) VGG Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pSEy-S7HyF9"
      },
      "source": [
        "# 위에서 정의한 블록들을 이용해 VGG 네트워크를 만들어보겠습니다.\n",
        "# 필터의 개수가 2의 n승의 값을 가지기 때문에 base_dim이란 변수를 추가해서 단순화 했습니다.\n",
        "# 현재 dog, cat 두 가지 클래스를 구분하려고 하기 때문에 num_classes=2로 설정했습니다.\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, base_dim, num_classes=2):\n",
        "        super(VGG, self).__init__()\n",
        "        self.feature = nn.Sequential(\n",
        "            conv_2_block(3,base_dim),\n",
        "            conv_2_block(base_dim,2*base_dim),\n",
        "            conv_4_block(2*base_dim,4*base_dim),\n",
        "            conv_4_block(4*base_dim,8*base_dim),\n",
        "            conv_4_block(8*base_dim,8*base_dim),            \n",
        "        )\n",
        "        self.fc_layer = nn.Sequential(\n",
        "            nn.Linear(8*base_dim * 7 * 7, 100),\n",
        "            nn.ReLU(True),                                                      # True 는 inplace 연산을 하겠다는 의미를 가집니다. inplace 연산은 결과값을 새로운 변수에 값을 저장하는 대신 기존의 데이터를 대체하는것을 의미합니다.\n",
        "            #nn.Dropout(),\n",
        "            nn.Linear(100, 20),\n",
        "            nn.ReLU(True),\n",
        "            #nn.Dropout(),\n",
        "            nn.Linear(20, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.feature(x)\n",
        "        x = x.view(x.size(0), -1)                                               # x.size(0)를 batch size로 바꿔도 같은 값입니다.\n",
        "        x = self.fc_layer(x)\n",
        "        return x\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsdmlK82HyGC"
      },
      "source": [
        "## 4. Optimizer & Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFpmR6WZHyGC",
        "outputId": "cacc436b-d2b5-434a-c327-b62705e1361c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# gpu가 사용 가능한 경우에는 device를 0번 gpu로 설정하고 불가능하면 cpu로 설정합니다.\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "# 앞서 정의한대로 vGG 클래스를 인스턴스화 하고 지정한 장치에 올립니다.\n",
        "model = VGG(base_dim=64).to(device)\n",
        "\n",
        "# 손실함수 및 최적화함수를 설정합니다.\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# 모델 자녀 노드의 이름과 모듈을 출력합니다.\n",
        "for i in model.named_children():\n",
        "    print(i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "('feature', Sequential(\n",
            "  (0): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU()\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (1): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU()\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (2): Sequential(\n",
            "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU()\n",
            "    (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): ReLU()\n",
            "    (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): ReLU()\n",
            "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (3): Sequential(\n",
            "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU()\n",
            "    (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): ReLU()\n",
            "    (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): ReLU()\n",
            "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (4): Sequential(\n",
            "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU()\n",
            "    (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): ReLU()\n",
            "    (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): ReLU()\n",
            "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "))\n",
            "('fc_layer', Sequential(\n",
            "  (0): Linear(in_features=25088, out_features=100, bias=True)\n",
            "  (1): ReLU(inplace=True)\n",
            "  (2): Linear(in_features=100, out_features=20, bias=True)\n",
            "  (3): ReLU(inplace=True)\n",
            "  (4): Linear(in_features=20, out_features=2, bias=True)\n",
            "))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuUHPn3AHyGG"
      },
      "source": [
        "## 5. Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvzZt_LDHyGH",
        "outputId": "b5037eb7-e840-42e7-88f1-a2cfd5017b52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for i in range(num_epoch):\n",
        "    for j,[image,label] in enumerate(train_loader):\n",
        "        x = image.to(device)\n",
        "        y_= label.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        output = model.forward(x)\n",
        "        loss = loss_func(output,y_)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if i % 10 ==0:\n",
        "        print(loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.6147, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.8312, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.5396, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(1.2828, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.4915, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(1.0808, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.4757, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.4497, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.4009, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.5036, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Review\n",
        "3*3이 메모리 효율을 포함, 채널 까지 봤을때 가장 효율적이다."
      ],
      "metadata": {
        "id": "e7mTMVQfc14g"
      }
    }
  ]
}