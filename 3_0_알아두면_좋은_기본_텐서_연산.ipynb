{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOGbMI0jC6kUP2UQNz1JebR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imhyunho99/2023-1--Deaplearning_Framework/blob/main/3_0_%EC%95%8C%EC%95%84%EB%91%90%EB%A9%B4_%EC%A2%8B%EC%9D%80_%EA%B8%B0%EB%B3%B8_%ED%85%90%EC%84%9C_%EC%97%B0%EC%82%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a51z77StQS5P",
        "outputId": "e2027150-d674-4868-92bb-c7eecc31ed68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (0.14.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision) (2.25.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision) (2.10)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install torch torchvision #torch, torch 시각화화\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) Random Numbers\n",
        "- torch.rand()\n",
        "- torch.randn()\n",
        "- torch.randint()"
      ],
      "metadata": {
        "id": "-GYdz7vkRwDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://pytorch.org/docs/stable/torch.html?highlight=rand#torch.rand\n",
        "# torch.rand(sizes) -> [0,1)\n",
        "# 0에서 1사이의 랜덤한 숫자\n",
        "x = torch.rand(2,3)\n",
        "print(\"rand:\", x)\n",
        "\n",
        "# https://pytorch.org/docs/stable/torch.html?highlight=randn#torch.randn\n",
        "# Random Normal\n",
        "# 정규분포에서 샘플링한 값\n",
        "x = torch.randn(2,3)\n",
        "print(\"randn:\", x)\n",
        "\n",
        "\n",
        "# https://pytorch.org/docs/stable/torch.html?highlight=rand#torch.randint\n",
        "# Random Integer\n",
        "# 시작과 끝 사이의 랜덤한 자연수\n",
        "x = torch.randint(2,5,size=(2,3))\n",
        "print(\"randint:\", x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mESbRQAlQn6P",
        "outputId": "4cca0b81-5c44-43e4-9366-7e698d2d3284"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rand: tensor([[0.8638, 0.8763, 0.9034],\n",
            "        [0.4483, 0.9358, 0.9809]])\n",
            "randn: tensor([[ 1.9468,  1.6434, -0.8423],\n",
            "        [-0.5557, -2.1574,  0.0406]])\n",
            "randint: tensor([[3, 3, 4],\n",
            "        [4, 2, 3]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 2) zeros & ones\n",
        "- torch.ones()\n",
        "- torch.ones_like()\n",
        "- torch.zeros()\n",
        "- torch.zeros_like("
      ],
      "metadata": {
        "id": "KurvBQhqSVBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://pytorch.org/docs/stable/torch.html?highlight=zeros#torch.zeros\n",
        "# torch.zeros(2,3) -> [[0,0,0],[0,0,0]]\n",
        "# 0으로 채워진 텐서\n",
        "x = torch.zeros(2,3)\n",
        "print(\"zeros:\", x)\n",
        "\n",
        "# https://pytorch.org/docs/stable/torch.html?highlight=zeros#torch.zeros_like\n",
        "# same shape tensor filled with zeros\n",
        "# 인자로 들어오는 텐서와 형태가 같은 0으로 채워진 텐서\n",
        "ref = torch.rand(4,5)\n",
        "x = torch.zeros_like(ref)\n",
        "print(\"zeros_like:\", x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEakO9N9ScZR",
        "outputId": "3388183e-02b8-4259-ddcc-679a43d366ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "zeros: tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "zeros_like: tensor([[0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) Tensor Data Type\n",
        "\n",
        "- tensor.type()\n",
        "- tensor.type_as()"
      ],
      "metadata": {
        "id": "BAfhqLUiS6n1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://pytorch.org/docs/stable/tensors.html?highlight=type#torch.Tensor.type\n",
        "# without dtype, returns type of tensor\n",
        "# tensor.type()은 해당 텐서의 타입을 리턴하고 type(tensor)는 토치의 텐서 클래스라는 것을 리턴함\n",
        "\n",
        "x = torch.rand(2,3)\n",
        "print(x)\n",
        "print(x.type())\n",
        "print(type(x))\n",
        "\n",
        "# with dtype, cast the object to the dtype\n",
        "# tensor.type()을 dtype과 함께 사용하면 텐서의 데이터 타입을 dtype에 넣어준 데이터 타입으로 바꿔줍니다.\n",
        "double_x = x.type(dtype=torch.DoubleTensor)\n",
        "print(double_x.type())\n",
        "\n",
        "# https://pytorch.org/docs/stable/tensors.html?highlight=type#torch.Tensor.type_as\n",
        "# 위의 type 함수와 유사하게 type_as라는 함수를 사용해 데이터타입을 바꿀 수 있습니다.\n",
        "int_x = x.type_as(torch.IntTensor())\n",
        "print(int_x.type())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09LlO6_tSwOy",
        "outputId": "60f5739e-b1f0-4a86-8a6d-5a4a40e55474"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.6865, 0.3973, 0.7378],\n",
            "        [0.1752, 0.2693, 0.1975]])\n",
            "torch.FloatTensor\n",
            "<class 'torch.Tensor'>\n",
            "torch.DoubleTensor\n",
            "torch.IntTensor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4) Numpy to Tensor, Tensor to Numpy\n",
        "- torch.from_numpy()\n",
        "- tensor.numpy()\n",
        "`"
      ],
      "metadata": {
        "id": "yiQ5gnovTrb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://pytorch.org/docs/stable/torch.html?highlight=from_numpy#torch.from_numpy\n",
        "# torch.from_numpy(ndarray) -> tensor\n",
        "# from_numpy 함수를 이용해 넘파이 배열을 토치텐서로 바꿀 수 있습니다.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "x1 = np.ndarray(shape=(2,3), dtype=int,buffer=np.array([1,2,3,4,5,6]))\n",
        "print(x1)\n",
        "x2 = torch.from_numpy(x1)\n",
        "print(x2)\n",
        "\n",
        "print(x2,x2.type())\n",
        "\n",
        "# https://pytorch.org/docs/stable/tensors.html?highlight=numpy#torch.Tensor.numpy\n",
        "# tensor.numpy() -> ndarray\n",
        "# 반대로 토치 텐서를 .numpy() 를 통해 넘파이 배열로 바꿀 수 있습니다.\n",
        "\n",
        "x3 = x2.numpy()\n",
        "print(x3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGYlqigJTxI8",
        "outputId": "1f64ae84-66da-4429-8b91-4ff6915409a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 2 3]\n",
            " [4 5 6]]\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]]) torch.LongTensor\n",
            "[[1 2 3]\n",
            " [4 5 6]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5) Tensor on CPU & GPU\n",
        "\n"
      ],
      "metadata": {
        "id": "yYPEO1ABT_4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 배열을 인자로 넣어 토치 텐서를 생성할 수 있습니다.\n",
        "\n",
        "x = torch.FloatTensor([[1,2,3],[4,5,6]])\n",
        "print(x)\n",
        "\n",
        "\n",
        "# https://pytorch.org/docs/stable/cuda.html?highlight=device#torch.cuda.device\n",
        "# device 함수를 사용해 텐서를 원하는 장치로 이동시킬 수 있습니다.\n",
        "cpu = torch.device('cpu')\n",
        "gpu = torch.device('cuda')\n",
        "\n",
        "# https://pytorch.org/docs/stable/cuda.html?highlight=available#torch.cuda.is_available\n",
        "# gpu가 사용 가능한지 체크해줍니다.\n",
        "if torch.cuda.is_available():\n",
        "  # https://pytorch.org/docs/stable/tensors.html?highlight=#torch.Tensor.to\n",
        "  # .to 함수를 이용해 지정한 장치로 이동시켜줍니다.\n",
        "  x_gpu = x.to(gpu)\n",
        "  print(x_gpu)\n",
        "  \n",
        "x_cpu = x_gpu.to(cpu)\n",
        "print(x_cpu)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qa6tQwcxURTH",
        "outputId": "9fcbb48e-0183-4d57-f2e8-db3e14d096e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.]])\n",
            "tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.]], device='cuda:0')\n",
            "tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6) Tensor Size"
      ],
      "metadata": {
        "id": "kmNG8Dp9UYBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://pytorch.org/docs/stable/tensors.html?highlight=size#torch.Tensor.size\n",
        "# size 함수를 이용해 텐서의 형태를 알 수 있습니다.\n",
        "x = torch.FloatTensor(10,12,3,3)\n",
        "x.size(), x.size()[1:2]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbJSCFjAVMMC",
        "outputId": "69b121dc-78e9-4c07-a931-bb35c9ae4113"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([10, 12, 3, 3]), torch.Size([12]))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Indexing, Joining, Slicing & Squeezing"
      ],
      "metadata": {
        "id": "TsQ10Y8WVQm0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) Indexing\n",
        "\n",
        "- torch.index_select()\n",
        "- torch.masked_select()"
      ],
      "metadata": {
        "id": "YIoM-c0mVs5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 임의의 텐서를 생성합니다.\n",
        "x = torch.randn(4,3)\n",
        "print(x)\n",
        "\n",
        "# middle 2 rows\n",
        "# 토치 텐서 역시 기존 파이썬 인덱싱을 똑같이 쓸 수 있습니다.\n",
        "print(x[1:3,:])\n",
        "\n",
        "\n",
        "# https://pytorch.org/docs/stable/torch.html?highlight=index_select#torch.index_select\n",
        "# index select along dimension 1\n",
        "# index_select 함수를 사용해 지정한 차원 기준으로 원하는 값들을 뽑아낼 수 있습니다.\n",
        "\n",
        "selected = torch.index_select(x,dim=1,index=torch.LongTensor([0,2]))\n",
        "print(selected)\n",
        "\n",
        "# https://pytorch.org/docs/stable/torch.html?highlight=masked_select#torch.masked_select\n",
        "# torch.masked_select(input, mask)\n",
        "# masked_select를 통해 뽑고자 하는 값들을 마스킹해서 선택할 수 있습니다.\n",
        "\n",
        "x = torch.randn(2,3)\n",
        "mask = torch.ByteTensor([[0,0,1],[0,1,0]])\n",
        "out = torch.masked_select(x,mask)\n",
        "\n",
        "print(x, mask, out, sep=\"\\n\\n\") # \"sep =\"  option"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rjAjqGKVwK8",
        "outputId": "79c78a55-d0b3-4849-aa96-c2b10912308f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.7420, -0.5884,  0.8216],\n",
            "        [ 2.0630, -1.6659,  0.6631],\n",
            "        [-1.5437, -4.7006,  1.4802],\n",
            "        [-1.1453, -1.6023,  0.2276]])\n",
            "tensor([[ 2.0630, -1.6659,  0.6631],\n",
            "        [-1.5437, -4.7006,  1.4802]])\n",
            "tensor([[-0.7420,  0.8216],\n",
            "        [ 2.0630,  0.6631],\n",
            "        [-1.5437,  1.4802],\n",
            "        [-1.1453,  0.2276]])\n",
            "tensor([[ 0.4521, -0.6687, -0.0555],\n",
            "        [-0.5319,  1.0292,  0.2130]])\n",
            "\n",
            "tensor([[0, 0, 1],\n",
            "        [0, 1, 0]], dtype=torch.uint8)\n",
            "\n",
            "tensor([-0.0555,  1.0292])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-9eeb1481e117>:23: UserWarning: masked_select received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at ../aten/src/ATen/native/TensorAdvancedIndexing.cpp:1729.)\n",
            "  out = torch.masked_select(x,mask)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) Joining\n",
        "- torch.cat()\n",
        "- torch.stack()"
      ],
      "metadata": {
        "id": "6hYkYIAdWBPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://pytorch.org/docs/stable/torch.html?highlight=torch%20cat#torch.cat\n",
        "# torch.cat(seq, dim=0) -> concatenate tensor along dim\n",
        "# cat 함수를 이용해 텐서를 원하는대로 붙일 수 있습니다.\n",
        "# cat은 concatenate의 약자입니다.\n",
        "\n",
        "x = torch.FloatTensor([[1,2,3],[4,5,6]])\n",
        "y = torch.FloatTensor([[-1,-2,-3],[-4,-5,-6]])\n",
        "z1 = torch.cat([x,y],dim=0) #dim = 0 --> 행\n",
        "z2 = torch.cat([x,y],dim=1) #dim = 1 --> 열열\n",
        "\n",
        "print(x,y,z1,z2,sep=\"\\n\\n\")\n",
        "\n",
        "# https://pytorch.org/docs/stable/torch.html?highlight=torch%20stack#torch.stack\n",
        "# torch.stack(sequence,dim=0) -> stack along new dim\n",
        "# stack 함수를 통해 텐서를 붙일수도 있습니다.\n",
        "\n",
        "x = torch.FloatTensor([[1,2,3],[4,5,6]])\n",
        "x_stack = torch.stack([x,x,x,x],dim=0) #3차원 텐서 생성으로 이어붙이기기\n",
        "\n",
        "print(x_stack,x_stack.size(),sep=\"\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5MiGBWRWFPH",
        "outputId": "1009255c-5480-48d0-f91d-5403904c333c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.]])\n",
            "\n",
            "tensor([[-1., -2., -3.],\n",
            "        [-4., -5., -6.]])\n",
            "\n",
            "tensor([[ 1.,  2.,  3.],\n",
            "        [ 4.,  5.,  6.],\n",
            "        [-1., -2., -3.],\n",
            "        [-4., -5., -6.]])\n",
            "\n",
            "tensor([[ 1.,  2.,  3., -1., -2., -3.],\n",
            "        [ 4.,  5.,  6., -4., -5., -6.]])\n",
            "tensor([[[1., 2., 3.],\n",
            "         [4., 5., 6.]],\n",
            "\n",
            "        [[1., 2., 3.],\n",
            "         [4., 5., 6.]],\n",
            "\n",
            "        [[1., 2., 3.],\n",
            "         [4., 5., 6.]],\n",
            "\n",
            "        [[1., 2., 3.],\n",
            "         [4., 5., 6.]]])\n",
            "\n",
            "torch.Size([4, 2, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) Slicing\n",
        "- torch.chunk()\n",
        "- torch.split()\n"
      ],
      "metadata": {
        "id": "e1yzMfKFWJRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://pytorch.org/docs/stable/torch.html?highlight=chunk#torch.chunk\n",
        "# torch.chunk(tensor, chunks, dim=0) -> tensor into number of chunks\n",
        "# chunk 함수를 통해 텐서를 원하는 chunk 개수만큼으로 분리할 수 있습니다.\n",
        "\n",
        "x_1, x_2 = torch.chunk(z1,2,dim=0)\n",
        "y_1, y_2, y_3 = torch.chunk(z1,3,dim=1)\n",
        "\n",
        "print(z1,x_1,x_2,z1,y_1,y_2,y_3,sep=\"\\n\")\n",
        "\n",
        "# https://pytorch.org/docs/stable/torch.html?highlight=split#torch.split\n",
        "# torch.split(tensor,split_size,dim=0) -> split into specific size\n",
        "# split 함수를 통해 원하는 사이즈로 텐서를 자를 수 있습니다.\n",
        "\n",
        "x1,x2 = torch.split(z1,2,dim=0)\n",
        "y1 = torch.split(z1,2,dim=1) \n",
        "\n",
        "print(z1,x1,x2,sep=\"\\n\")\n",
        "\n",
        "print(\"\\nThis is y1:\")\n",
        "for i in y1:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrp-6N2GXRGG",
        "outputId": "a48cc9e5-e491-4d30-d1a0-38344b487f5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.,  2.,  3.],\n",
            "        [ 4.,  5.,  6.],\n",
            "        [-1., -2., -3.],\n",
            "        [-4., -5., -6.]])\n",
            "tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.]])\n",
            "tensor([[-1., -2., -3.],\n",
            "        [-4., -5., -6.]])\n",
            "tensor([[ 1.,  2.,  3.],\n",
            "        [ 4.,  5.,  6.],\n",
            "        [-1., -2., -3.],\n",
            "        [-4., -5., -6.]])\n",
            "tensor([[ 1.],\n",
            "        [ 4.],\n",
            "        [-1.],\n",
            "        [-4.]])\n",
            "tensor([[ 2.],\n",
            "        [ 5.],\n",
            "        [-2.],\n",
            "        [-5.]])\n",
            "tensor([[ 3.],\n",
            "        [ 6.],\n",
            "        [-3.],\n",
            "        [-6.]])\n",
            "tensor([[ 1.,  2.,  3.],\n",
            "        [ 4.,  5.,  6.],\n",
            "        [-1., -2., -3.],\n",
            "        [-4., -5., -6.]])\n",
            "tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.]])\n",
            "tensor([[-1., -2., -3.],\n",
            "        [-4., -5., -6.]])\n",
            "\n",
            "This is y1:\n",
            "tensor([[ 1.,  2.],\n",
            "        [ 4.,  5.],\n",
            "        [-1., -2.],\n",
            "        [-4., -5.]])\n",
            "tensor([[ 3.],\n",
            "        [ 6.],\n",
            "        [-3.],\n",
            "        [-6.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4) squeezing"
      ],
      "metadata": {
        "id": "9PfqNC4cXWwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://pytorch.org/docs/stable/torch.html?highlight=squeeze#torch.squeeze\n",
        "# torch.squeeze(input,dim=None) -> reduce dim by 1\n",
        "# squeeze 함수를 통해 길이가 1인 차원들을 압축시킬 수 있습니다.\n",
        "\n",
        "x1 = torch.FloatTensor(10,1,3,1,4)\n",
        "x2 = torch.squeeze(x1)\n",
        "\n",
        "print(x1.size(),x2.size(),sep=\"\\n\")\n",
        "\n",
        "# https://pytorch.org/docs/stable/torch.html#torch.unsqueeze\n",
        "# torch.unsqueeze(input,dim=None) -> add dim by 1\n",
        "# squeeze와 반대로 unsqueeze를 통해 차원을 늘릴수 있습니다.\n",
        "\n",
        "x1 = torch.FloatTensor(10,3,4)\n",
        "x2 = torch.unsqueeze(x1,dim=0)\n",
        "\n",
        "print(x1.size(),x2.size(),sep=\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPQLfIiQY3wo",
        "outputId": "b541705a-32b2-403a-f9e6-1156e4744f4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 1, 3, 1, 4])\n",
            "torch.Size([10, 3, 4])\n",
            "torch.Size([10, 3, 4])\n",
            "torch.Size([1, 10, 3, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Initialization\n",
        "- 텐서 값 초기화"
      ],
      "metadata": {
        "id": "7RAw5ND9Y8lA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# _(underbar) refers to in-place operation -> https://discuss.pytorch.org/t/what-is-in-place-operation/16244\n",
        "import torch.nn.init as init\n",
        "\n",
        "# https://pytorch.org/docs/stable/nn.html?highlight=init%20uniform#torch.nn.init.uniform_\n",
        "# uniform 분포를 따라 텐서를 초기화 할 수 있습니다.\n",
        "x1 = init.uniform_(torch.FloatTensor(3,4),a=0,b=9) \n",
        "\n",
        "# https://pytorch.org/docs/stable/nn.html?highlight=init%20uniform#torch.nn.init.normal_\n",
        "# 정규 분포를 따라 텐서를 초기화 할 수 있습니다.\n",
        "x2 = init.normal_(torch.FloatTensor(3,4),std=0.2)\n",
        "\n",
        "# https://pytorch.org/docs/stable/nn.html?highlight=init%20uniform#torch.nn.init.constant_\n",
        "# 지정한 값으로 텐서를 초기화 할 수 있습니다.\n",
        "x3 = init.constant_(torch.FloatTensor(3,4),3.1415)\n",
        "\n",
        "print(\"uniform:\", x1 ,\"\\nnormla:\",x2,\"\\nconstant(pi):\",x3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TI9TDyo9Y-18",
        "outputId": "df553896-df91-410e-e46f-8b16667cf9dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "uniform: tensor([[6.2776, 2.4912, 3.1313, 4.3700],\n",
            "        [2.7677, 4.8571, 7.9920, 5.3643],\n",
            "        [7.0058, 3.9472, 3.0055, 5.0449]]) \n",
            "normla: tensor([[ 0.1385,  0.2692, -0.1271,  0.3586],\n",
            "        [-0.1821,  0.1808, -0.4354,  0.1143],\n",
            "        [-0.1528,  0.0675, -0.0607, -0.1284]]) \n",
            "constant(pi): tensor([[3.1415, 3.1415, 3.1415, 3.1415],\n",
            "        [3.1415, 3.1415, 3.1415, 3.1415],\n",
            "        [3.1415, 3.1415, 3.1415, 3.1415]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Math Operations"
      ],
      "metadata": {
        "id": "8IYZRW_EZAGG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) Arithmetic operations\n"
      ],
      "metadata": {
        "id": "si1PiAYxZGii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://pytorch.org/docs/stable/torch.html?highlight=add#torch.add\n",
        "# torch.add()\n",
        "# 더하기 연산\n",
        "\n",
        "x1 = torch.FloatTensor([[1,2,3],[4,5,6]])\n",
        "x2 = torch.FloatTensor([[1,2,3],[4,5,6]])\n",
        "add = torch.add(x1,x2)\n",
        "\n",
        "print(x1,x2,add,x1+x2,x1-x2,sep=\"\\n\")\n",
        "\n",
        "# torch.add() broadcasting\n",
        "# 더하기 연산의 브로드캐스팅\n",
        "\n",
        "x1 = torch.FloatTensor([[1,2,3],[4,5,6]])\n",
        "x2 = torch.add(x1,10) #모든 항목에 10씩 뿌리기\n",
        "\n",
        "print(x1,x2,x1+10,x2-10,sep=\"\\n\")\n",
        "\n",
        "# https://pytorch.org/docs/stable/torch.html?highlight=add#torch.mul\n",
        "# torch.mul() -> element-wise multiplication \n",
        "# size better match\n",
        "# 곱하기 연산입니다. 각각의 요소간 곱이기 때문에 사이즈가 일치해야 연산이 가능합니다.\n",
        "# 행렬 곱X 자리끼리 곱하기 연산\n",
        "x1 = torch.FloatTensor([[1,2,3],[4,5,6]])\n",
        "x2 = torch.FloatTensor([[1,2,3],[4,5,6]])\n",
        "x3 = torch.mul(x1,x2)\n",
        "\n",
        "print(x3)\n",
        "\n",
        "# torch.mul() -> broadcasting\n",
        "# 곱하기 연산의 브로드캐스팅\n",
        "\n",
        "x1 = torch.FloatTensor([[1,2,3],[4,5,6]])\n",
        "x2 = x1*10\n",
        "#모든 항목에 10씩 곱하기\n",
        "print(x2)\n",
        "\n",
        "# https://pytorch.org/docs/stable/torch.html?highlight=add#torch.div\n",
        "# torch.div() -> size better match\n",
        "# 나누기 연산. 각각의 요소간 나누기이기 때문에 사이즈가 일치해야 연산이 가능합니다.\n",
        "\n",
        "x1 = torch.FloatTensor([[1,2,3],[4,5,6]])\n",
        "x2 = torch.FloatTensor([[1,2,3],[4,5,6]])\n",
        "x3 = torch.div(x1,x2)\n",
        "\n",
        "print(x3)\n",
        "\n",
        "# torch.div() -> broadcasting\n",
        "# 나누기 연산의 브로드캐스팅\n",
        "\n",
        "x1 = torch.FloatTensor([[1,2,3],[4,5,6]])\n",
        "print(x1/5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsZ2fgbWZJZ5",
        "outputId": "9793d8c8-ee8e-48a2-c2b9-2016c0f49f51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.]])\n",
            "tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.]])\n",
            "tensor([[ 2.,  4.,  6.],\n",
            "        [ 8., 10., 12.]])\n",
            "tensor([[ 2.,  4.,  6.],\n",
            "        [ 8., 10., 12.]])\n",
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.]])\n",
            "tensor([[11., 12., 13.],\n",
            "        [14., 15., 16.]])\n",
            "tensor([[11., 12., 13.],\n",
            "        [14., 15., 16.]])\n",
            "tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.]])\n",
            "tensor([[ 1.,  4.,  9.],\n",
            "        [16., 25., 36.]])\n",
            "tensor([[10., 20., 30.],\n",
            "        [40., 50., 60.]])\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "tensor([[0.2000, 0.4000, 0.6000],\n",
            "        [0.8000, 1.0000, 1.2000]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://pytorch.org/docs/stable/torch.html?highlight=add#torch.pow\n",
        "# torch.pow(input,exponent)\n",
        "# 파워 연산(x의 n승)\n",
        "\n",
        "x1 = torch.FloatTensor(3,4)\n",
        "x2 = torch.pow(x1,2),x1**2\n",
        "print(x2)\n",
        "\n",
        "# https://pytorch.org/docs/stable/torch.html?highlight=add#torch.exp\n",
        "# torch.exp(tensor,out=None) \n",
        "# exponential 연산\n",
        "\n",
        "x1 = torch.FloatTensor(3,4)\n",
        "x2 = torch.exp(x1)\n",
        "print(x2)\n",
        "# https://pytorch.org/docs/stable/torch.html?highlight=add#torch.log\n",
        "# torch.log(input, out=None) -> natural logarithm\n",
        "# 로그 연산\n",
        "\n",
        "x1 = torch.FloatTensor(3,4)\n",
        "x2 = torch.log(x1)\n",
        "print(x2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FS3zZI4jZLJL",
        "outputId": "57cb7cf9-69aa-47e4-88fe-5f9a2874f84f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([[9.2694e+04, 0.0000e+00, 1.1755e-38, 2.8900e+00],\n",
            "        [4.0000e+00, 3.1506e+00, 1.1755e-38, 3.3306e+00],\n",
            "        [0.0000e+00, 3.5156e+00, 4.0000e+00, 3.6100e+00]]), tensor([[9.2694e+04, 0.0000e+00, 1.1755e-38, 2.8900e+00],\n",
            "        [4.0000e+00, 3.1506e+00, 1.1755e-38, 3.3306e+00],\n",
            "        [0.0000e+00, 3.5156e+00, 4.0000e+00, 3.6100e+00]]))\n",
            "tensor([[1.4394, 1.0000,    inf, 1.0000],\n",
            "        [1.0000, 1.0000, 1.0000, 1.0000],\n",
            "        [1.0000, 1.0000, 1.0000, 1.0000]])\n",
            "tensor([[ -1.0100, -92.8884,   5.7159,     -inf],\n",
            "        [  0.0000,   0.0000,   0.0000,   0.0000],\n",
            "        [  0.0000,   0.0000,   0.0000,   1.2837]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) Matrix Operations"
      ],
      "metadata": {
        "id": "Xpa_j0lPbUSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://pytorch.org/docs/stable/torch.html?highlight=add#torch.mm\n",
        "# torch.mm(mat1, mat2) -> matrix multiplication\n",
        "# 행렬곱 연산\n",
        "\n",
        "x1 = torch.FloatTensor(3,4)\n",
        "x2 = torch.FloatTensor(4,5)\n",
        "\n",
        "torch.mm(x1,x2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDJVqt8Pbacv",
        "outputId": "a068abb5-43ee-48fc-9382-0f23de3a8017"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.3266e-01, 1.3534e-38, 9.1539e+04, 0.0000e+00, 5.5718e-33],\n",
              "        [6.2719e-38, 0.0000e+00, 3.2646e-17, 0.0000e+00, 0.0000e+00],\n",
              "        [0.0000e+00, 0.0000e+00, 5.4431e-41, 0.0000e+00, 0.0000e+00]])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://pytorch.org/docs/stable/torch.html?highlight=add#torch.bmm\n",
        "# torch.bmm(batch1, batch2) -> batch matrix multiplication\n",
        "# 배치 행렬곱 연산. 맨 앞에 batch 차원은 무시하고 뒤에 요소들로 행렬곱을 합니다.\n",
        "\n",
        "x1 = torch.FloatTensor(10,3,4)\n",
        "x2 = torch.FloatTensor(10,4,5)\n",
        "\n",
        "torch.bmm(x1,x2).size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhX5X05ObjJu",
        "outputId": "d23b6038-c09b-4079-9bfe-152b0b855c3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 3, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://pytorch.org/docs/stable/torch.html?highlight=add#torch.dot\n",
        "# torch.dot(tensor1,tensor2) -> dot product of two tensor\n",
        "# 두 텐서간의 프로덕트 연산\n",
        "\n",
        "x1 = torch.tensor([2, 3])\n",
        "x2 = torch.tensor([2, 1])\n",
        "\n",
        "torch.dot(x1,x2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0Ub1IUVcRG4",
        "outputId": "eb2dcd4d-5ea2-4340-ffa6-6a8ab2cf2e6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(7)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://pytorch.org/docs/stable/torch.html?highlight=add#torch.t\n",
        "# torch.t(matrix) -> transposed matrix\n",
        "# 행렬의 전치\n",
        "\n",
        "x1 = torch.tensor([[1,2],[3,4]])\n",
        "print(x1,x1.t(),sep=\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWSwySoscSU1",
        "outputId": "b9a558a5-0d66-4375-b0d9-8cb296cec37f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 2],\n",
            "        [3, 4]])\n",
            "tensor([[1, 3],\n",
            "        [2, 4]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://pytorch.org/docs/stable/torch.html?highlight=add#torch.transpose\n",
        "# torch.transpose(input,dim0,dim1) -> transposed matrix\n",
        "# 차원을 지정할 수 있는 행렬의 전치 연산\n",
        "\n",
        "x1 = torch.FloatTensor(10,3,4)\n",
        "print(x1.size(), torch.transpose(x1,1,2).size(), x1.transpose(1,2).size(),sep=\"\\n\")\n",
        "\n",
        "print(\"\\n 0번 차원과 2번 차원 전치\\n\")\n",
        "print(x1.size(), torch.transpose(x1,0,2).size(), x1.transpose(0,2).size(), sep=\"\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SosOJ2_cTef",
        "outputId": "d2f3a7ab-58d9-4d47-d2f2-a0d0d81521d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 3, 4])\n",
            "torch.Size([10, 4, 3])\n",
            "torch.Size([10, 4, 3])\n",
            "\n",
            " 0번 차원과 2번 차원 전치\n",
            "\n",
            "torch.Size([10, 3, 4])\n",
            "\n",
            "torch.Size([4, 3, 10])\n",
            "\n",
            "torch.Size([4, 3, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NQ8dBBCacUhq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}