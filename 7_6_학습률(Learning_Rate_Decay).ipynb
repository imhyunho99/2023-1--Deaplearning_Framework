{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imhyunho99/2023-1--Deaplearning_Framework/blob/main/7_6_%ED%95%99%EC%8A%B5%EB%A5%A0(Learning_Rate_Decay).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJYIFF4hS26-"
      },
      "source": [
        "# 학습률 스케쥴러 Learning Rate Scheduler\n",
        "\n",
        "- https://pytorch.org/docs/stable/optim.html?highlight=lr_scheduler\n",
        "- optim.lr_scheduler.StepLR()\n",
        "- optim.lr_scheduler.MultiStepLR()\n",
        "- optim.lr_scheduler.ExponentialLR()\n",
        "- optim.lr_scheduler.ReduceLROnPlateau()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ud97XP9wTKPb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b886e05-5ff3-418e-cf77-3cc625e66086"
      },
      "source": [
        "!pip install torch torchvision"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1bHk0pOS27A"
      },
      "source": [
        "## 1. Settings\n",
        "### 1) Import required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqUOqmyjS27B"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.init as init\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TL13CYomS27G"
      },
      "source": [
        "### 2) Set hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRup0iidS27H"
      },
      "source": [
        "batch_size = 256\n",
        "learning_rate = 0.001\n",
        "num_epoch = 10"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbrVD36_S27M"
      },
      "source": [
        "## 2. Data\n",
        "\n",
        "### 1) Download Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCEG7ki8S27M"
      },
      "source": [
        "mnist_train = dset.MNIST(\"./\", train=True, transform=transforms.ToTensor(), target_transform=None, download=True)\n",
        "mnist_test = dset.MNIST(\"./\", train=False, transform=transforms.ToTensor(), target_transform=None, download=True)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlmnh82CS27Q"
      },
      "source": [
        "### 2) Check Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtzlypO2S27R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a95f6144-85fe-4faa-ed2d-3d0bfe97eff5"
      },
      "source": [
        "print(mnist_train.__getitem__(0)[0].size(), mnist_train.__len__())\n",
        "mnist_test.__getitem__(0)[0].size(), mnist_test.__len__()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 28, 28]) 60000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 28, 28]), 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XecioAb1S27Y"
      },
      "source": [
        "### 3) Set DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4c-SKpUSS27Z"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(mnist_train,batch_size=batch_size, shuffle=True,num_workers=2,drop_last=True)\n",
        "test_loader = torch.utils.data.DataLoader(mnist_test,batch_size=batch_size, shuffle=False,num_workers=2,drop_last=True)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHzyE9nYS27c"
      },
      "source": [
        "## 3. Model & Optimizer\n",
        "\n",
        "### 1) CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "do9LBe5NS27e"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN,self).__init__()\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Conv2d(1,16,3,padding=1),  # 28 x 28\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16,32,3,padding=1), # 28 x 28\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2),            # 14 x 14\n",
        "            nn.Conv2d(32,64,3,padding=1), # 14 x 14\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2)             #  7 x 7\n",
        "        )\n",
        "        self.fc_layer = nn.Sequential(\n",
        "            nn.Linear(64*7*7,100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100,10)\n",
        "        )        \n",
        "        \n",
        "    def forward(self,x):\n",
        "        out = self.layer(x)\n",
        "        out = out.view(batch_size,-1)\n",
        "        out = self.fc_layer(out)\n",
        "        return out"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJ3o5UsdS27i"
      },
      "source": [
        "### 2) Loss func & Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5fb_i07S27k",
        "outputId": "80e14dc2-122b-4cbd-9e94-21c9e94878bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "model = CNN().to(device)\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#weight decay\n",
        "\n",
        "# 지정한 스텝 단위로 학습률에 감마를 곱해 학습률을 감소시킵니다.\n",
        "#scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma= 0.99)       \n",
        "\n",
        "# 지정한 스텝 지점(예시에서는 10,30,80)마다 학습률에 감마를 곱해줍니다.\n",
        "#scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[10,30,80], gamma= 0.1)  \n",
        "\n",
        "# 매 epoch마다 학습률에 감마를 곱해줍니다.\n",
        "#scheduler = lr_scheduler.ExponentialLR(optimizer, gamma= 0.99)                             \n",
        "\n",
        "# https://pytorch.org/docs/stable/optim.html?highlight=lr_scheduler#torch.optim.lr_scheduler.ReduceLROnPlateau\n",
        "# 지정한 메트릭으로 측정한 값이 더 나아지지 않으면 학습률을 감소시킵니다. ex) 정확도, dice score 등등\n",
        "# 이 스케쥴러에는 다양한 인자가 들어가는데 각각의 역할은 도큐먼트를 참고 바랍니다.\n",
        "# 여기서는 patience 즉, 지정한 값이 줄어들지 않을때 몇 epoch 만큼을 지켜볼 것인지를 1로 낮춰놨기 때문에 매 epoch 마다 학습률이 감소하는것을 확인할 수 있습니다.\n",
        "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer,threshold=1,patience=1,mode='min')    \n",
        "\n",
        "# 참고 https://www.geeksforgeeks.org/python-dir-function/\n",
        "print(dir(scheduler))\n",
        "print(dir(optimizer))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_init_is_better', '_reduce_lr', '_reset', 'best', 'cooldown', 'cooldown_counter', 'eps', 'factor', 'in_cooldown', 'is_better', 'last_epoch', 'load_state_dict', 'min_lrs', 'mode', 'mode_worse', 'num_bad_epochs', 'optimizer', 'patience', 'state_dict', 'step', 'threshold', 'threshold_mode', 'verbose']\n",
            "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_cuda_graph_capture_health_check', '_init_group', '_optimizer_step_code', '_optimizer_step_post_hooks', '_optimizer_step_pre_hooks', '_patch_step_function', '_warned_capturable_if_run_uncaptured', '_zero_grad_profile_name', 'add_param_group', 'defaults', 'load_state_dict', 'param_groups', 'profile_hook_step', 'register_step_post_hook', 'register_step_pre_hook', 'state', 'state_dict', 'step', 'zero_grad']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HqEIbfIS27p"
      },
      "source": [
        "## 4. Train "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "BJHK9ylqS27r",
        "outputId": "63544871-1903-4274-8e66-09e75cb2a5e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for i in range(num_epoch):\n",
        "    # ReduceLRONPlateau 빼고는 아래의 코드를 사용하세요\n",
        "    #scheduler.step()  \n",
        "    for j,[image,label] in enumerate(train_loader):\n",
        "        x = image.to(device)\n",
        "        y_= label.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        output = model.forward(x)\n",
        "        loss = loss_func(output,y_)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    # ReduceLRONPlateau 만 해당됩니다. 이 코드에서는 손실이 줄어들지 않으면 학습률을 낮추도록 만들어놨습니다.\n",
        "    scheduler.step(loss)      \n",
        "    \n",
        "    if i % 10 == 0:\n",
        "        print(loss)   \n",
        "            \n",
        "    #print(\"Epoch: {}, Learning Rate: {}\".format(i,scheduler.get_lr()))  \n",
        "    print(\"Epoch: {}, Learning Rate: {}\".format(i,scheduler.optimizer.state_dict()['param_groups'][0]['lr']))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.3049, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0, Learning Rate: 0.001\n",
            "Epoch: 1, Learning Rate: 0.0001\n",
            "Epoch: 2, Learning Rate: 0.0001\n",
            "Epoch: 3, Learning Rate: 1e-05\n",
            "Epoch: 4, Learning Rate: 1e-05\n",
            "Epoch: 5, Learning Rate: 1.0000000000000002e-06\n",
            "Epoch: 6, Learning Rate: 1.0000000000000002e-06\n",
            "Epoch: 7, Learning Rate: 1.0000000000000002e-07\n",
            "Epoch: 8, Learning Rate: 1.0000000000000002e-07\n",
            "Epoch: 9, Learning Rate: 1.0000000000000004e-08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtTeg5BuS27v"
      },
      "source": [
        "## 5. Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTUDYg2_S27w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f403ff15-6ca4-4f1b-e20a-fe45ff78ce44"
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "  for image,label in test_loader:\n",
        "      x = image.to(device)\n",
        "      y_= label.to(device)\n",
        "\n",
        "      output = model.forward(x)\n",
        "      _,output_index = torch.max(output,1)\n",
        "\n",
        "      total += label.size(0)\n",
        "      correct += (output_index == y_).sum().float()\n",
        "\n",
        "  print(\"Accuracy of Test Data: {}\".format(100*correct/total))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Test Data: 10.316506385803223\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Batch size가 클수록 lr을 더 과감하게 바꿀 수 있다#\n",
        "\n"
      ],
      "metadata": {
        "id": "JaOfw6puXW5W"
      }
    }
  ]
}