{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imhyunho99/2023-1--Deaplearning_Framework/blob/main/6_3_%EA%B0%84%EB%8B%A8%ED%95%9C_%EB%AC%B8%EC%9E%90_%EC%88%9C%ED%99%98_%EC%8B%A0%EA%B2%BD%EB%A7%9D_%EC%8B%A4%EC%8A%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QGnnbwC37wF",
        "outputId": "ec4f872d-e7b3-43f1-ccc3-d6e1642f4992",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install torch torchvision"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.1+cu118)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwV7Yzdz4Lla"
      },
      "source": [
        "# 단순한 문자 RNN을 만들어보겠습니다.\n",
        "\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcjSLaS337wU"
      },
      "source": [
        "# 하이퍼파라미터 설정\n",
        "\n",
        "n_hidden = 35 \n",
        "lr = 0.01\n",
        "epochs = 1000"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HE0ZIXVp37wJ"
      },
      "source": [
        "# 사용하는 문자는 영어 소문자 및 몇가지 특수문자로 제한했습니다.\n",
        "# alphabet(0-25), space(26), ... , start(0), end(1)\n",
        "\n",
        "string = \"hello pytorch. RNN practice\"\n",
        "chars =  \"abcdefghijklmnopqrstuvwxyz ABCDEFGHIJKLMNOPQRSTUVXYZ ?!.,:;01\"\n",
        "\n",
        "# 문자들을 리스트로 바꾸고 이의 길이(=문자의 개수)를 저장해놓습니다.\n",
        "char_list = [i for i in chars]\n",
        "n_letters = len(char_list)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhYTZTsuGygL"
      },
      "source": [
        "# 문자를 그대로 쓰지않고 one-hot 벡터로 바꿔서 연산에 쓰도록 하겠습니다.\n",
        "\n",
        "#Start = [0 0 0 … 1 0]\n",
        "#a =     [1 0 0 … 0 0]\n",
        "#b =     [0 1 0 … 0 0]\n",
        "#c =     [0 0 1 … 0 0]\n",
        "#...\n",
        "#end =   [0 0 0 … 0 1]"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0oGDc_o37wN"
      },
      "source": [
        "# 문자열을 one-hot 벡터의 스택으로 만드는 함수\n",
        "# abc -> [[1 0 0 … 0 0],\n",
        "#         [0 1 0 … 0 0],\n",
        "#         [0 0 1 … 0 0]]\n",
        "\n",
        "def string_to_onehot(string):\n",
        "    # 먼저 시작 토큰과 끝 토큰을 만들어줍니다.\n",
        "    start = np.zeros(shape=n_letters ,dtype=int)\n",
        "    end = np.zeros(shape=n_letters ,dtype=int)\n",
        "    start[-2] = 1\n",
        "    end[-1] = 1\n",
        "    # 여기서부터는 문자열의 문자들을 차례대로 받아서 진행합니다.\n",
        "    for i in string:\n",
        "        # 먼저 문자가 몇번째 문자인지 찾습니다.\n",
        "        # a:0, b:1, c:2,...\n",
        "        idx = char_list.index(i)\n",
        "        # 0으로만 구성된 배열을 만들어줍니다.\n",
        "        # [0 0 0 … 0 0]\n",
        "        zero = np.zeros(shape=n_letters ,dtype=int)\n",
        "        # 해당 문자 인데스만 1로 바꿔줍니다.\n",
        "        # b: [0 1 0 … 0 0]\n",
        "        zero[idx]=1\n",
        "        # start와 새로 생긴 zero를 붙이고 이를 start에 할당합니다.\n",
        "        # 이게 반복되면 start에는 문자를 one-hot 벡터로 바꾼 배열들이 점점 쌓여가게 됩니다.\n",
        "        start = np.vstack([start,zero])\n",
        "    # 문자열이 다 끝나면 쌓아온 start와 end를 붙여줍니다.\n",
        "    output = np.vstack([start,end])\n",
        "    return output"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNSZj7Tv37wQ"
      },
      "source": [
        "# One-hot 벡터를 문자로 바꿔주는 함수 \n",
        "# [1 0 0 ... 0 0] -> a \n",
        "# https://pytorch.org/docs/stable/tensors.html?highlight=numpy#torch.Tensor.numpy\n",
        "\n",
        "def onehot_to_word(onehot_1):\n",
        "    # 텐서를 입력으로 받아 넘파이 배열로 바꿔줍니다.\n",
        "    onehot = torch.Tensor.numpy(onehot_1)\n",
        "    # one-hot 벡터의 최대값(=1) 위치 인덱스로 문자를 찾습니다.\n",
        "    return char_list[onehot.argmax()]"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMI_b1MO37wR"
      },
      "source": [
        "# RNN with 1 hidden layer\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNN, self).__init__()\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        \n",
        "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
        "        self.act_fn = nn.Tanh()\n",
        "    \n",
        "    def forward(self, input, hidden):\n",
        "        # 입력과 hidden state를 cat함수로 붙여줍니다.\n",
        "        combined = torch.cat((input, hidden), 1)\n",
        "        # 붙인 값을 i2h 및 i2o에 통과시켜 hidden state는 업데이트, 결과값은 계산해줍니다.\n",
        "        hidden = self.act_fn(self.i2h(combined))\n",
        "        output = self.i2o(combined)\n",
        "        return output, hidden\n",
        "    \n",
        "    # 아직 입력이 없을때(t=0)의 hidden state를 초기화해줍니다. \n",
        "    def init_hidden(self):\n",
        "        return torch.zeros(1, self.hidden_size)\n",
        "    \n",
        "rnn = RNN(n_letters, n_hidden, n_letters)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dseHJCeb37wW"
      },
      "source": [
        "# 손실함수와 최적화함수를 설정해줍니다.\n",
        "\n",
        "loss_func = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(rnn.parameters(), lr=lr)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9x44BXX37wZ",
        "outputId": "29d37923-d3d9-4f07-ebd1-f9512ec591de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# train\n",
        "\n",
        "# 문자열을 onehot 벡터로 만들고 이를 토치 텐서로 바꿔줍니다.\n",
        "# 또한 데이터타입도 학습에 맞게 바꿔줍니다.\n",
        "one_hot = torch.from_numpy(string_to_onehot(string)).type_as(torch.FloatTensor())\n",
        "\n",
        "for i in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    # 학습에 앞서 hidden state를 초기화해줍니다.\n",
        "    hidden = rnn.init_hidden()\n",
        "    \n",
        "    # 문자열 전체에 대한 손실을 구하기 위해 total_loss라는 변수를 만들어줍니다. \n",
        "    total_loss = 0\n",
        "    for j in range(one_hot.size()[0]-1):\n",
        "        # 입력은 앞에 글자 \n",
        "        # pyotrch 에서 p y t o r c\n",
        "        input_ = one_hot[j:j+1,:]\n",
        "        # 목표값은 뒤에 글자\n",
        "        # pytorch 에서 y t o r c h\n",
        "        target = one_hot[j+1]\n",
        "        output, hidden = rnn.forward(input_, hidden)\n",
        "        \n",
        "        loss = loss_func(output.view(-1),target.view(-1))\n",
        "        total_loss += loss\n",
        "\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if i % 10 == 0:\n",
        "        print(total_loss)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.6452, grad_fn=<AddBackward0>)\n",
            "tensor(0.1865, grad_fn=<AddBackward0>)\n",
            "tensor(0.0596, grad_fn=<AddBackward0>)\n",
            "tensor(0.0185, grad_fn=<AddBackward0>)\n",
            "tensor(0.0071, grad_fn=<AddBackward0>)\n",
            "tensor(0.0030, grad_fn=<AddBackward0>)\n",
            "tensor(0.0012, grad_fn=<AddBackward0>)\n",
            "tensor(0.0004, grad_fn=<AddBackward0>)\n",
            "tensor(0.0002, grad_fn=<AddBackward0>)\n",
            "tensor(9.8097e-05, grad_fn=<AddBackward0>)\n",
            "tensor(1.9678e-05, grad_fn=<AddBackward0>)\n",
            "tensor(9.7985e-06, grad_fn=<AddBackward0>)\n",
            "tensor(3.1179e-06, grad_fn=<AddBackward0>)\n",
            "tensor(1.7719e-06, grad_fn=<AddBackward0>)\n",
            "tensor(4.1456e-07, grad_fn=<AddBackward0>)\n",
            "tensor(1.6011e-07, grad_fn=<AddBackward0>)\n",
            "tensor(1.3283e-06, grad_fn=<AddBackward0>)\n",
            "tensor(7.8578e-06, grad_fn=<AddBackward0>)\n",
            "tensor(3.5420e-05, grad_fn=<AddBackward0>)\n",
            "tensor(5.9772e-06, grad_fn=<AddBackward0>)\n",
            "tensor(8.9709e-07, grad_fn=<AddBackward0>)\n",
            "tensor(1.0858e-06, grad_fn=<AddBackward0>)\n",
            "tensor(7.0712e-07, grad_fn=<AddBackward0>)\n",
            "tensor(3.2653e-08, grad_fn=<AddBackward0>)\n",
            "tensor(6.7889e-08, grad_fn=<AddBackward0>)\n",
            "tensor(1.1987e-08, grad_fn=<AddBackward0>)\n",
            "tensor(8.4823e-10, grad_fn=<AddBackward0>)\n",
            "tensor(1.6686e-09, grad_fn=<AddBackward0>)\n",
            "tensor(4.2926e-10, grad_fn=<AddBackward0>)\n",
            "tensor(2.4589e-10, grad_fn=<AddBackward0>)\n",
            "tensor(1.8308e-10, grad_fn=<AddBackward0>)\n",
            "tensor(1.0216e-11, grad_fn=<AddBackward0>)\n",
            "tensor(1.9105e-11, grad_fn=<AddBackward0>)\n",
            "tensor(1.3428e-11, grad_fn=<AddBackward0>)\n",
            "tensor(3.3271e-12, grad_fn=<AddBackward0>)\n",
            "tensor(2.5053e-12, grad_fn=<AddBackward0>)\n",
            "tensor(3.0838e-12, grad_fn=<AddBackward0>)\n",
            "tensor(7.1238e-10, grad_fn=<AddBackward0>)\n",
            "tensor(1.1384e-06, grad_fn=<AddBackward0>)\n",
            "tensor(0.0005, grad_fn=<AddBackward0>)\n",
            "tensor(3.8046e-05, grad_fn=<AddBackward0>)\n",
            "tensor(3.2971e-05, grad_fn=<AddBackward0>)\n",
            "tensor(8.7596e-06, grad_fn=<AddBackward0>)\n",
            "tensor(4.3426e-06, grad_fn=<AddBackward0>)\n",
            "tensor(1.3121e-06, grad_fn=<AddBackward0>)\n",
            "tensor(6.3800e-07, grad_fn=<AddBackward0>)\n",
            "tensor(2.0467e-07, grad_fn=<AddBackward0>)\n",
            "tensor(5.1926e-08, grad_fn=<AddBackward0>)\n",
            "tensor(1.3389e-08, grad_fn=<AddBackward0>)\n",
            "tensor(7.7756e-09, grad_fn=<AddBackward0>)\n",
            "tensor(4.3615e-09, grad_fn=<AddBackward0>)\n",
            "tensor(6.2203e-10, grad_fn=<AddBackward0>)\n",
            "tensor(1.0663e-09, grad_fn=<AddBackward0>)\n",
            "tensor(9.8808e-09, grad_fn=<AddBackward0>)\n",
            "tensor(1.2221e-06, grad_fn=<AddBackward0>)\n",
            "tensor(0.0003, grad_fn=<AddBackward0>)\n",
            "tensor(5.8102e-05, grad_fn=<AddBackward0>)\n",
            "tensor(1.5616e-05, grad_fn=<AddBackward0>)\n",
            "tensor(3.3098e-06, grad_fn=<AddBackward0>)\n",
            "tensor(1.6577e-06, grad_fn=<AddBackward0>)\n",
            "tensor(7.4912e-07, grad_fn=<AddBackward0>)\n",
            "tensor(2.4472e-07, grad_fn=<AddBackward0>)\n",
            "tensor(4.5786e-08, grad_fn=<AddBackward0>)\n",
            "tensor(1.6460e-08, grad_fn=<AddBackward0>)\n",
            "tensor(4.7804e-09, grad_fn=<AddBackward0>)\n",
            "tensor(3.7121e-09, grad_fn=<AddBackward0>)\n",
            "tensor(2.0559e-09, grad_fn=<AddBackward0>)\n",
            "tensor(3.1810e-10, grad_fn=<AddBackward0>)\n",
            "tensor(1.4050e-10, grad_fn=<AddBackward0>)\n",
            "tensor(5.5166e-10, grad_fn=<AddBackward0>)\n",
            "tensor(1.3919e-07, grad_fn=<AddBackward0>)\n",
            "tensor(9.8990e-05, grad_fn=<AddBackward0>)\n",
            "tensor(6.6612e-05, grad_fn=<AddBackward0>)\n",
            "tensor(2.1351e-05, grad_fn=<AddBackward0>)\n",
            "tensor(7.1800e-06, grad_fn=<AddBackward0>)\n",
            "tensor(1.7549e-06, grad_fn=<AddBackward0>)\n",
            "tensor(9.4382e-07, grad_fn=<AddBackward0>)\n",
            "tensor(4.8473e-07, grad_fn=<AddBackward0>)\n",
            "tensor(1.0459e-07, grad_fn=<AddBackward0>)\n",
            "tensor(4.0333e-08, grad_fn=<AddBackward0>)\n",
            "tensor(1.5187e-08, grad_fn=<AddBackward0>)\n",
            "tensor(5.7164e-09, grad_fn=<AddBackward0>)\n",
            "tensor(2.6762e-09, grad_fn=<AddBackward0>)\n",
            "tensor(6.3822e-10, grad_fn=<AddBackward0>)\n",
            "tensor(1.4936e-09, grad_fn=<AddBackward0>)\n",
            "tensor(5.1212e-08, grad_fn=<AddBackward0>)\n",
            "tensor(8.5856e-06, grad_fn=<AddBackward0>)\n",
            "tensor(0.0001, grad_fn=<AddBackward0>)\n",
            "tensor(3.2880e-05, grad_fn=<AddBackward0>)\n",
            "tensor(4.8834e-06, grad_fn=<AddBackward0>)\n",
            "tensor(3.6731e-06, grad_fn=<AddBackward0>)\n",
            "tensor(1.0986e-06, grad_fn=<AddBackward0>)\n",
            "tensor(5.5735e-07, grad_fn=<AddBackward0>)\n",
            "tensor(1.7031e-07, grad_fn=<AddBackward0>)\n",
            "tensor(5.1107e-08, grad_fn=<AddBackward0>)\n",
            "tensor(2.3032e-08, grad_fn=<AddBackward0>)\n",
            "tensor(5.0211e-09, grad_fn=<AddBackward0>)\n",
            "tensor(3.3371e-09, grad_fn=<AddBackward0>)\n",
            "tensor(4.1223e-09, grad_fn=<AddBackward0>)\n",
            "tensor(1.1493e-07, grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8suWP0r37wf",
        "outputId": "49da7a2f-b35e-4ff7-e0ce-31e51c829432",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# test \n",
        "# hidden state 는 처음 한번만 초기화해줍니다.\n",
        "\n",
        "start = torch.zeros(1,n_letters)\n",
        "start[:,-2] = 1\n",
        "\n",
        "with torch.no_grad():\n",
        "    hidden = rnn.init_hidden()\n",
        "    # 처음 입력으로 start token을 전달해줍니다.\n",
        "    input_ = start\n",
        "    # output string에 문자들을 계속 붙여줍니다.\n",
        "    output_string = \"\"\n",
        "\n",
        "    # 원래는 end token이 나올때 까지 반복하는게 맞으나 끝나지 않아서 string의 길이로 정했습니다.\n",
        "    for i in range(len(string)):\n",
        "        output, hidden = rnn.forward(input_, hidden)\n",
        "        # 결과값을 문자로 바꿔서 output_string에 붙여줍니다.\n",
        "        output_string += onehot_to_word(output.data)\n",
        "        # 또한 이번의 결과값이 다음의 입력값이 됩니다.\n",
        "        input_ = output\n",
        "\n",
        "print(output_string)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello pytorch. RNN practice\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Oks8JryMcCG"
      },
      "source": [
        "## 이 파일을 다 보셨으면 [참고]LSTM 연습코드를 먼저 보시고 6.4절로 넘어가시는걸 추천드립니다."
      ]
    }
  ]
}